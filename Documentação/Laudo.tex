\documentclass[12pt,a4paper,final]{article}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[brazil]{minitoc}\usepackage[a4paper,left=2cm,right=2cm,top=4cm,bottom=4cm]{geometry}
\usepackage[portuguese]{babel}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{xcolor}
\lstset { %
    language=C++,
    backgroundcolor=\color{black!5}, % set backgroundcolor
    basicstyle=\footnotesize,% basic font setting
}

\newcommand\tab[1][1cm]{\hspace*{#1}}

\author{Luan Bodner do Rosário}
\title{Relatório - Compiladores}
\begin{document}
\maketitle
\section*{Introdução}
Este documento tem como objetivo especificar o processo de implementação da primeira parte de um compilador(análise léxica e análise sintática) para a disciplina de Compiladores.
Para a implementação do programa, foi utilizada a linguagem de programação C++ e a biblioteca \textit{boost} para auxíliar na varredura do arquivo.\\
As saídas e entradas desse programa estão armazenadas em arquivos dados prévia à execução do compilador.
O restante deste documento está organizado da seguinte forma:
\begin{itemize}
\item Análise Léxica : Funcionamento da varredura do arquivo e descrição das bibliotecas utilizadas e criadas para a captura das marcas do arquivo T++.
\item Análise Sintática: Descrição do método utilizado para criação da AST ,análise da estrutura dos \textit{tokens} e métodos de tratamento de erros.
\item Gramática : Definição da gramática BNF utilizada para a criação da análise sintática e descrição da classe de tratamento de erros do compilador.
\end{itemize}

\section*{Análise Léxica}
Para o processo de varredura, não foi utilizada nenhuma ferramenta auxiliar para a construção da lista de \textit{tokens}. Para que as marcas fossem reconhecidas no arquivo, foi utilizada a biblioteca \textit{boost/regex}.\\

Os tipos de marcas foram armazenadas em um \textit{enum} na biblioteca \textit{Token.h}. Essa lista de marcas são definidas dessa forma:
\\

\begin{lstlisting}
    typedef enum {
    
        IF, THEN, OTHERWISE, END, REPEAT,
        FLOAT, VOID, UNTIL, READ, WRITE,
        INTEGER, COMMENTS, RETURN, SUM, SUBTRACTION,
        MULTIPLICATION, DIVISION, EQUAL, COMMA, ATTRIBUTION,
        LESS_THAN, BIGGER_THAN, LESS_EQUAL, BIGGER_EQUAL, OPEN,
        CLOSE, NUMBER_INTEGER, NUMBER_FLOAT, DOUBLE_POINT, IDENTIFIER
    } TokenType;
\end{lstlisting}

Quando dado \textit{token} é analisado, o seu conteúdo é armazenado em um vetor de estruturas \textit{tokens}

Os \textit{tokens} são analisados de acordo com expressões regulares. As mais importantes listadas a seguir:

\begin{lstlisting}
	static std::vector<Token::Token> tokens;
\end{lstlisting}

A classe \textit{Token.h} é dada pela seguinte estrutura:

\begin{lstlisting}
	std::string tokenName;
	TokenType token;
	int tokenColumn;
	int tokenLine;
\end{lstlisting}

O arquivo em que o código fonte é armazenado é lido linha por linha até que todos os caracteres sejam analisados e aceitos pela varredura, caso algum erro aconteça, a execução é finalizada.
A varredura do arquivo é feita pela classe \textit{LexicalAnalyzer}.

\section*{Análise Sintática}
A análise sintática deste projeto foi feita utilizando algoritmos \textit{top-down} de análise recursiva descendente sem auxílio de ferramentas para a verificação da estrutura do programa.\\
A gramática BNF foi transformada para se adequar à analise descendente (retirada da recursão à esquerda).\\
Utilizando o algoritmo de descida recursiva, a gramática e a estrutura do programa ficaram bastante semelhantes, já que cada não terminal da gramática se tornou um método dentro do programa que avançava na lista de \textit{tokens} conforme a leitura avançava.\\

A biblioteca responsável pela criação da árvore e análise é chamada de \textit{SyntaxAnalyzer.h} e  conta com apenas dois atributos:

\begin{lstlisting}
	Lex::LexicalAnalyzer lexer;
	Tree::Tree syntaxTree;
\end{lstlisting}

A análise léxica está portanto diretamente ligada a análise sintática e não exitirá sem uma instância da análise sintática, assim como a classe \textit{Tree.h}, que é definida pela seguinte estrutura:

\begin{lstlisting}
	std::string exp;
	Token::Token token;
	int active;
	std::vector<Tree*> children;
\end{lstlisting}

Conforme os \textit{tokens} são analisados e se encaixam na gramática, eles também são inseridos na árvore de acordo com a sua colocação dentro do programa e sua necessidade. Desta forma, montamos uma árvore abstrata onde apenas as folhas representam valores diretamente do programa.\\

Os erros gerados durante a execução do programa foram especificados na classe \textit{CompilerErrors.h} (tanto erros léxicos quanto erros sintáticos), onde cada método mostra uma mensagem adequada quanto a coluna e a linha do programa onde determinado erro aconteceu. Como por exemplo:

\begin{lstlisting}
void CompilerErrors::unidentifiedTokenError(int Token, Token::Token token) {

	std::cout << "Unidentified Token Error; "
				<< "Received : "
                << intToString(Token)
                << "; Expected : "
                << token.tokenTypeToString()
                << "; Name : " << token.getTokenName();
                
	exit(EXIT_FAILURE);
}
\end{lstlisting}

Portanto, a cada erro encontrado, o programa mostra a mensagem de erro e é finalizado, o que gera o problema de não conseguir mostrar mais de um erro por compilação.

\section*{Gramática}

<type> ::= 'vazio'\\
\tab\tab | 'inteiro'\\
\tab\tab | 'flutuante'\\
\\
<variableDec> ::= <type> ':' 'id' \\
\\
<operationsExp> ::= <equalityExp>\\
\\
<equalityExp> ::= <relationalExp> <equalityExpTransformed>\\
\\
<equalityExpTransformed> ::= NULL\\
\tab\tab\tab\tab\tab | '=' <relationalExp> <equalityExpTransformed>\\
\\
<relationalExp> ::= <additiveExp> <relationalExpTransformed>\\
\\
<relationalExpTransformed> ::= NULL\\
\tab \tab \tab \tab \tab| < <additiveExp> <relationalExpTransformed>\\
\tab \tab \tab \tab \tab| > <additiveExp> <relationalExpTransformed>\\
\tab \tab \tab \tab \tab| <= <additiveExp> <relationalExpTransformed>\\
\tab \tab \tab \tab \tab| >= <additiveExp> <relationalExpTransformed>\\
\\
<additiveExp> ::= <multiplicativeExp> <additiveExpTransformed>\\
\\
<additiveExpTransformed> ::= NULL\\
\tab \tab \tab \tab \tab| + <multiplicativeExp> <additiveExpTransformed>\\
\tab \tab \tab \tab \tab| - <multiplicativeExp> <additiveExpTransformed> \\		
\\
<multiplicativeExp> ::= <factor> <multiplicationExpTransformed>\\
\\
<multiplicationExpTransformed> ::= NULL\\
\tab \tab \tab \tab \tab \tab| * <factor> <multiplicationExpTransformed>\\
\tab \tab \tab \tab \tab \tab| / <factor> <multiplicationExpTransformed>\\
\\
<factor> ::= '(' <operationsExp> ')'\\
\tab \tab | 'numberFloat'\\
\tab \tab | 'numberInt'\\
\tab \tab | 'id'\\
\\
<prototypeDef> ::= '(' <paramFunction> ')'\\
\\
<paramFunction> ::= NULL\\
\tab \tab \tab | <variableDec> ',' <paramFunction>\\
\tab \tab \tab | <variableDec>\\
\\
<functionDec> ::= <type> 'id' <prototypeDef> <compoundStmt> 'fim'\\
\\
<prototypeCall> ::= NULL\\
\tab \tab \tab | <operationsExp> ',' <protypeCall>\\
\tab \tab \tab | <operationsExp>\\
\\
<functionCall> ::= 'id' '(' <paramCall> ')' \\
\\
<iterationExp> ::= 'repita' <compoundStmt> 'até' <operationsExp> 'fim'\\
\\
<selectionExp> ::= 'se' <operationsExp> 'então' <compoundStmt> 'fim'\\
\tab \tab \tab | 'se' <operationsExp> 'então' <compoundStmt> 'senão'\\ <compoundStmt> 'fim'\\
\\
<ioTypes> ::= 'id'\\
\tab \tab | 'numberInt'\\
\tab \tab | 'numberFloat'\\
\tab \tab | <functionCall>\\
\\
<attributionExp> ::= 'id' ':=' <operationsExp>\\
\\
<returnCom> ::= 'retorna' '(' <operationsExp> ')'\\
\\
<readCom> ::= 'leia' '(' 'id' ')'\\
\\
<writeCom> ::= 'escreve' '(' <operationsExp> ')'\\
\\
<compoundStmt> ::= <expression> <compoundStmt>\\
\tab \tab \tab \tab | <expression>\\
\\
<expression> ::= <selectionExp>\\
\tab \tab \tab | <iterationExp>\\
\tab \tab \tab | <functionCall>\\
\tab \tab \tab | <readCom>\\
\tab \tab \tab | <writeCom>\\
\tab \tab \tab | <returnCom>\\
\tab \tab \tab | <variableDec>\\
\tab \tab \tab | <attributionExp>\\
\newpage
\section*{Mudança na Linguagem}

Foram feitas algumas modificações na sintátixe da linguagem, como a remoção de múltiplos retornos(é possível criar apenas um retorno por função) e as expressões numéricas e chamadas de funções dentro de expressões deve estrar entre parenteses, pois isso facilita a analise da expressão.

\section*{Análise Semântica}

Para fazer a análise Semântica da linguagem, sendo essa a "terceira" passada pelo código foi criada a classe \textit{SemanticAnalysis}. Essa classe por sua vez passa pela AST e gera uma tabela de símbolos, como exemplificada a seguir:

\begin{lstlisting}

0, principal / vazio / Function / 0
1, ret / inteiro / Used / Initialized
1, f / inteiro / Used / Initialized
0, fatorial / vazio / Function / 1 / inteiro
0, n / inteiro / Global / Unused / Not Initialized

\end{lstlisting}

Essa tabela é representada no programa como uma \textit{hashtable}, cuja estrutura é definida da seguinte maneira:

\begin{lstlisting}

typedef std::pair<int, std::string> ScopeName;
typedef std::vector<std::string> vectorString;

boost::unordered_map<ScopeName, vectorString> symbolTable;

\end{lstlisting}

As colunas armazenadas nessa tabela são:
\begin{itemize}
\item Escopo(chave)
\item Nome(chave)
\item Tipo
\end{itemize}
A partir daqui, a tabela se diferencia entre funções e variaveis, sendo que para funções, as próximas colunas são:
\begin{itemize}
\item \textit{Flag} definindo se é uma função
\item Número de parâmetros
\item Tipo de cada um dos parâmetros
\end{itemize}
Para variáveis:
\begin{itemize}
\item \textit{Flag} definindo se é global
\item \textit{Flag} definindo se foi ou não utilizada
\item \textit{Flag} definindo se foi ou não inicializada
\end{itemize}

Não foi utilizada nenhuma técnica em especial para geração dessa tabela, sendo que ela foi preenchida conforme os dados foram descobertos na AST.

Além disso, a adição da análise semântica levou à modificação da classe de Erros, que foi atualizada agora para gerar os \textit{warnings} e erros da anpalise semântica, o cabeçalho das novas funções adicionadas são:

\begin{lstlisting}
        void declarationScopeError(Token::Token);
        void variableNotDeclaredError(Token::Token);
        void functionCallError(Token::Token);
        void functionCallScopeError(Token::Token);
        void variableNotDefinedError(Token::Token);
        void variableVoidError(Token::Token);
        void expressionTypeError(Token::Token);
        void returnIgnoredWarning(Token::Token);
        void voidAttributionError(Token::Token);
        void functionWithoutReturnWarning(std::string);
        void returnMayBeIgnoredWarning(Token::Token);
        void variableNotUsedWarning(std::string, std::string);
        void mainDeclarationError();
        void tooManyReturnsError(Token::Token);
\end{lstlisting}

Além dessas adições, o esquema de mensagem de erro e \textit{warnings} também foi modificada de forma que elas sejam visualmente mais simples de perceber e produzam mais informações para o programador. Entre as melhorias citadas estão:

\begin{itemize}
\item Mensagens com cores
\item printar os \textit{tokens} onde o erro se encontra
\end{itemize}

A estrutura da AST não foi modificada, e a geração de código é feita em cima da tabela de simbolos gerada e da própria árvore.

\section*{Geração de Código}
Para a geração de código, inicialmente foi utilizada a classe \textit{CodeGeneration}, que foi refatorada em uma nova classe \textit{llvmCodeGeneration}, devido aos diversos erros gerados. A classe inicial então foi reutilizada quando adequada.

Como váriaveis da classe, foram armazenadas as referencias dos módulos responsáveis pela adição de instruções e contexto dos módulos.

\begin{lstlisting}

        /* Current Builder */
        llvm::IRBuilder<> * builder;

        /* tiny Module */
        llvm::Module * module;

        /* Current function */
        llvm::Function * function;

        /* Current basic block */
        llvm::BasicBlock * block;
\end{lstlisting}

Conforme as variáveis são definidas ou declaradas, sejam elas globais ou locais, elas são armazenadas em duas \textit{hashtables} separadas, para que a origem da variável seja distinguível.

\begin{lstlisting}

    	typedef boost::unordered_map<ScopeName, vectorString> SymbolTable;
      
        /* Alloca Instruction table */
        boost::unordered_map<ScopeName, llvm::AllocaInst*> variablesHash;

        /* Function Parameters table */
        boost::unordered_map<ScopeName, llvm::AllocaInst*> paramHash;
\end{lstlisting}

A geração de código não foi concluída, portanto não foi possível aplicar nenhuma forma de otimização de código.
Foram concluidas apenas as seguintes operações na linguagem :
\begin{itemize}
\item Atribuição
\item Declaração de globais
\item Declaração de funções
\item Chamada de funções
\item Alocação de variáveis locais
\item Operações de soma, subtração, multiplicação e divisão com valores inteiros ou flutuantes
\item Retorno nas funções
\end{itemize}

\section*{Execução}
Para executar o compilador, deve-se executar o seguinte comando na linha de comando após a compilação:

\begin{lstlisting}

./compilador <nome da entrada> <lista de tokens> <AST> <tabela de simbolos>

\end{lstlisting}

\end{document}